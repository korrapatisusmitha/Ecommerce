# ğŸ›’ E-Commerce Data Pipeline using PySpark & Databricks

## ğŸ“Œ Project Overview
This project demonstrates an **end-to-end batch data engineering pipeline** for an e-commerce use case using **Databricks, PySpark, Spark SQL, and Delta Lake**, following the **Medallion Architecture (Bronzeâ€“Silverâ€“Gold)**.

The pipeline processes **3 months of CSV-based transactional data**, transforms it into analytics-ready datasets, and showcases how a **unified lakehouse platform** simplifies modern data engineering workflows.

---

## ğŸ¯ Objective
This project solves challenges associated with **legacy data architectures**, such as:
- ğŸ’¸ High service and infrastructure costs  
- ğŸ”— Scattered ETL logic across multiple tools  
- ğŸ§© Complex and nonâ€“beginner-friendly setups  
- ğŸ“‰ Limited scalability and inefficient cloud usage  

### Why Databricks?
Databricks was chosen as it represents a **best-practice unified platform** widely adopted across the data industry:
- âš¡ Highly scalable and cloud-native  
- ğŸ§  Supports **Data Engineers, Analysts, and AI/ML Engineers** on the same platform  
- ğŸ› ï¸ Handles **cluster lifecycle management**, allowing focus on **code and business logic**  
- ğŸ§± Provides powerful features like **Delta Lake**, **Time Travel**, **Unity Catalog**, and ACID transactions  

---

## ğŸ“Š Data Description
- **Source**: CSV files (synthetic e-commerce data)
- **Time Range**: 3 months (monthly data)
- **Tables**:
  - ğŸ·ï¸ `brands`
  - ğŸ“‚ `category`
  - ğŸ‘¤ `customers`
  - ğŸ“… `date`
  - ğŸ›’ `order_items`
  - ğŸ“¦ `product`

---

## ğŸ—ï¸ Architecture
The project follows the **Medallion Architecture** implemented using **Delta Lake**.


### ğŸ¥‰ Bronze Layer (Raw)
- Raw CSV ingestion
- Minimal transformations
- Schema enforcement using Delta tables

### ğŸ¥ˆ Silver Layer (Cleansed & Transformed)
- Data cleaning and standardization
- Deduplication
- Joins across multiple tables
- Business-level transformations

### ğŸ¥‡ Gold Layer (Analytics Ready)
- Aggregated datasets
- Business KPIs and metrics
- Optimized for analytics and reporting

---

## ğŸ§° Technologies Used
- ğŸš€ Databricks  
- ğŸ PySpark  
- ğŸ“œ Spark SQL  
- ğŸ’¾ Delta Lake  
- ğŸ—ï¸ Medallion Architecture  

---

## ğŸ”„ Key Transformations & Logic
- ğŸ”— Joining fact and dimension tables  
- ğŸ§¹ Data cleansing and deduplication  
- ğŸ“Š Aggregations to generate:
  - Total revenue  
  - Order counts  
  - Product-level performance  

---

## ğŸ“ˆ Output & Use Cases
- ğŸ“Š Analytics-ready Gold tables  
- ğŸ‘©â€ğŸ’¼ Can be consumed by:
  - Data Analysts (BI & reporting)
  - Business stakeholders
  - Downstream ML pipelines

---

## â–¶ï¸ How to Run the Project
- ğŸ““ Notebook-based implementation in Databricks
- Upload CSV files to Databricks File System (DBFS)
- Run notebooks in sequence:
  1. Bronze ingestion
  2. Silver transformations
  3. Gold aggregations

---

## ğŸš€ Future Enhancements
- ğŸ”„ Incremental data loading
- ğŸŒŠ Structured Streaming
- ğŸ“Š BI dashboard integration
- ğŸ¤– Machine learning use cases

---

## ğŸ“š What I Learned
- ğŸ§± Designing scalable pipelines using **Medallion Architecture**
- âš¡ Writing efficient **PySpark and Spark SQL** transformations
- ğŸ’¾ Leveraging **Delta Lake** features such as ACID transactions and Time Travel
- â˜ï¸ Understanding how Databricks abstracts infrastructure complexity
- ğŸ—ï¸ Building projects aligned with **real-world data engineering best practices**

---

## ğŸ§¾ Conclusion
This project strengthened my understanding of **modern data engineering workflows** and demonstrated how Databricks enables teams to build **scalable, maintainable, and cost-effective lakehouse solutions**.

Using a unified platform allowed me to focus on **data modeling and business logic**, while Databricks handled infrastructure management, scalability, and performance optimization.

---

âœ¨ *This project is designed as a portfolio-ready implementation aligned with real-world data engineering practices.*
